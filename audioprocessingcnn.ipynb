{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Audio Processing using CNN in TensorFlow\n",
        "\n",
        "In this notebook, we will walk through the basics of audio processing using a Convolutional Neural Network (CNN) in TensorFlow. We'll cover the following steps:\n",
        "1. Load and preprocess the audio data\n",
        "2. Convert audio data to spectrograms\n",
        "3. Build a CNN model\n",
        "4. Train the model\n",
        "5. Evaluate the model\n",
        "\n",
        "## 1. Load and Preprocess Audio Data\n",
        "\n",
        "We'll use the UrbanSound8K dataset for this example. First, let's install the necessary libraries and download the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "njmxZLFllN7Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91IIsy1neU4W"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install required libraries\n",
        "!pip install tensorflow numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install tensorflow librosa wget\n"
      ],
      "metadata": {
        "id": "TLR423ZylUoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4sKyV-afjq7O"
      },
      "outputs": [],
      "source": [
        "# Download the UrbanSound8K dataset\n",
        "import os\n",
        "import wget\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tDrPTVV1dJSv",
        "outputId": "84d73ed3-dbe3-40a0-bc58-cdfd34560619"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'UrbanSound8K.tar.gz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "url = r'https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz'\n",
        "output_dir = r'UrbanSound8K.tar.gz'\n",
        "wget.download(url, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DK5Z13rDs87e"
      },
      "outputs": [],
      "source": [
        "# Extract the dataset\n",
        "import tarfile\n",
        "tar = tarfile.open(output_dir, \"r:gz\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. Convert Audio Data to Spectrograms\n",
        "We will use Librosa to load audio files and convert them into spectrograms.\n",
        "\n"
      ],
      "metadata": {
        "id": "ue1x46Mr3DjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "trhLxx3uOLaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "def audio_to_spectrogram(file_path, max_pad_len=128):\n",
        "    y, sr = librosa.load(file_path, sr=None)\n",
        "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
        "\n",
        "    # Padding or truncating the spectrogram to a fixed size\n",
        "    if spectrogram_db.shape[1] > max_pad_len:\n",
        "        spectrogram_db = spectrogram_db[:, :max_pad_len]\n",
        "    else:\n",
        "        pad_width = max_pad_len - spectrogram_db.shape[1]\n",
        "        spectrogram_db = np.pad(spectrogram_db, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "\n",
        "    return spectrogram_db, sr\n",
        "\n",
        "# Example: Convert an audio file to a spectrogram and plot it\n",
        "example_file = 'UrbanSound8K/audio/fold1/101415-3-0-2.wav'\n",
        "spectrogram, sr = audio_to_spectrogram(example_file)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(spectrogram, x_axis='time', y_axis='mel', sr=sr)\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Mel-frequency spectrogram')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-ym90ZOv2_7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Build a CNN Model\n",
        "We will build a simple CNN model for audio classification\n"
      ],
      "metadata": {
        "id": "QAn7B6pOOOj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_cnn_model(input_shape):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dense(10, activation='softmax'))  # Assuming 10 classes in the dataset\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "input_shape = (128, 128, 1)  # Example input shape\n",
        "model = build_cnn_model(input_shape)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "EFbdsTnw3AcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Train the Model\n",
        "We'll prepare the dataset and train the model."
      ],
      "metadata": {
        "id": "rhDz0yY3OQ8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def prepare_dataset(folder, test_size=0.2, max_pad_len=128):\n",
        "    X = []\n",
        "    y = []\n",
        "    for subdir, dirs, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            if file.endswith('.wav'):\n",
        "                file_path = os.path.join(subdir, file)\n",
        "                spectrogram, sr = audio_to_spectrogram(file_path, max_pad_len=max_pad_len)\n",
        "                spectrogram = np.expand_dims(spectrogram, axis=-1)  # Add channel dimension\n",
        "                label = int(file.split('-')[1])  # Assuming label is in the filename\n",
        "                X.append(spectrogram)\n",
        "                y.append(label)\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = prepare_dataset('UrbanSound8K/audio')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "HytC4DYl3GvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Evaluate the Model\n",
        "Finally, we'll evaluate the model on the test set."
      ],
      "metadata": {
        "id": "vQpjjZONOTG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.2f}')\n"
      ],
      "metadata": {
        "id": "u9JGZHL53KJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c25f6bb7-7fad-45c9-d3a7-84b076fff44e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7256 - loss: 1.0394\n",
            "Test accuracy: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides a basic walkthrough of audio processing using a CNN in TensorFlow. You can further experiment with different model architectures, hyperparameters, and data augmentation techniques to improve the performance."
      ],
      "metadata": {
        "id": "Z7y45swo3OLG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nzds_pqB3MnU"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}