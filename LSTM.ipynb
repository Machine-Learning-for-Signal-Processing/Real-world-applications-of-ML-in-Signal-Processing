{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Applications of ML in Signal Processing: Long Short-Term Memory (LSTM) using TensorFlow\n",
    "\n",
    "In this notebook, we will explore how to use machine learning, specifically TensorFlow, for time series prediction using Long Short-Term Memory (LSTM) networks. This notebook is designed for beginners with basic knowledge of Python and machine learning concepts.\n",
    "\n",
    "## Objectives\n",
    "1. Understand the basics of time series data.\n",
    "2. Preprocess time series data for machine learning.\n",
    "3. Build and train an LSTM network using TensorFlow.\n",
    "4. Evaluate the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding Time Series Data\n",
    "\n",
    "Time series data is a sequence of data points collected or recorded at specific time intervals. Examples of time series data include stock prices, weather data, and sensor readings.\n",
    "\n",
    "Key concepts include:\n",
    "- **Trend**: The overall direction in which the data is moving over a long period.\n",
    "- **Seasonality**: Regular, repeating patterns in the data, often tied to calendar cycles.\n",
    "- **Noise**: Random variations in the data that cannot be explained by the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Loading and Preprocessing Time Series Data\n",
    "\n",
    "We'll use the `pandas` library to load and preprocess time series data. For this example, we'll use a dataset of daily minimum temperatures in Melbourne, Australia.\n",
    "\n",
    "First, let's install the required libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv'\n",
    "data = pd.read_csv(url, parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll visualize the time series data to understand its patterns and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data, label='Daily Minimum Temperatures')\n",
    "plt.title('Daily Minimum Temperatures in Melbourne')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Preparing the Data for LSTM\n",
    "\n",
    "LSTM networks require the input data to be in a specific format. We'll split the data into training and testing sets, normalize it, and reshape it to be suitable for LSTM input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(data_scaled) * 0.8)\n",
    "train, test = data_scaled[:train_size], data_scaled[train_size:]\n",
    "\n",
    "# Function to create dataset with look_back\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        X.append(dataset[i:(i + look_back), 0])\n",
    "        y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "look_back = 10\n",
    "X_train, y_train = create_dataset(train, look_back)\n",
    "X_test, y_test = create_dataset(test, look_back)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], look_back, 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], look_back, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Building and Training the LSTM Model\n",
    "\n",
    "We'll build a simple LSTM model using TensorFlow and train it on our prepared dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(look_back, 1)),\n",
    "    LSTM(50),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluating the Model\n",
    "\n",
    "We'll evaluate the model's performance by comparing its predictions to the actual values in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "# Invert predictions\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "y_train_actual = scaler.inverse_transform([y_train])\n",
    "y_test_actual = scaler.inverse_transform([y_test])\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data.index[:train_size + look_back], y_train_actual[0], label='Actual Training Data')\n",
    "plt.plot(data.index[train_size + look_back:], y_test_actual[0], label='Actual Testing Data')\n",
    "plt.plot(data.index[:train_size + look_back], train_predict, label='Training Predictions')\n",
    "plt.plot(data.index[train_size + look_back:], test_predict, label='Testing Predictions')\n",
    "plt.title('LSTM Model Predictions')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we covered the basics of time series data, preprocessing it for machine learning, building and training an LSTM model using TensorFlow, and evaluating the model's performance. This is just a starting point, and there are many more advanced techniques and models to explore in the field of time series analysis and machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
    "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

