{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91IIsy1neU4W"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install required libraries\n",
        "!pip install tensorflow numpy matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define theoretical concepts\n",
        "# A GAN consists of two networks: the Generator and the Discriminator.\n",
        "# The Generator creates fake images, and the Discriminator tries to distinguish between real and fake images.\n",
        "# They are trained together in a zero-sum game: the Generator tries to improve its fake images to fool the Discriminator,\n",
        "# while the Discriminator tries to get better at distinguishing real from fake.\n"
      ],
      "metadata": {
        "id": "njmxZLFllN7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Reshape, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.datasets import mnist\n"
      ],
      "metadata": {
        "id": "TLR423ZylUoR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4sKyV-afjq7O"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=100))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(784, activation='tanh'))\n",
        "    model.add(Reshape((28, 28, 1)))\n",
        "    return model\n",
        "\n",
        "# Define the Discriminator model\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28, 1)))\n",
        "    model.add(Dense(128))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "def build_gan(generator, discriminator):\n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tDrPTVV1dJSv"
      },
      "outputs": [],
      "source": [
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(optimizer='adam', loss='binary_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK5Z13rDs87e"
      },
      "outputs": [],
      "source": [
        "# Display the models\n",
        "generator.summary()\n",
        "discriminator.summary()\n",
        "gan.summary()\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(real_images, batch_size):\n",
        "    # Generate fake images\n",
        "    noise = tf.random.normal([batch_size, 100])\n",
        "    fake_images = generator(noise, training=True)\n",
        "\n",
        "    # Train Discriminator\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "        real_output = discriminator(real_images, training=True)\n",
        "        fake_output = discriminator(fake_images, training=True)\n",
        "        disc_loss_real = tf.keras.losses.binary_crossentropy(tf.ones_like(real_output), real_output)\n",
        "        disc_loss_fake = tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
        "        disc_loss = disc_loss_real + disc_loss_fake\n",
        "\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    # Train Generator\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        fake_images = generator(noise, training=True)\n",
        "        fake_output = discriminator(fake_images, training=True)\n",
        "        gen_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "    return disc_loss, gen_loss"
      ],
      "metadata": {
        "id": "uFOZeCFG0UlQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "siCrgY7kuyZR"
      },
      "outputs": [],
      "source": [
        "def train_gan(epochs, batch_size=64):\n",
        "    # Load and preprocess MNIST dataset\n",
        "    (X_train, _), (_, _) = mnist.load_data()\n",
        "    X_train = (X_train / 127.5) - 1.0\n",
        "    X_train = np.expand_dims(X_train, axis=-1)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Select a random batch of real images\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        real_images = X_train[idx]\n",
        "\n",
        "        # Perform a training step\n",
        "        disc_loss, gen_loss = train_step(real_images, batch_size)\n",
        "\n",
        "        # Print progress\n",
        "        if epoch % 1000 == 0:\n",
        "            print(f\"{epoch} [D loss: {np.mean(disc_loss):.4f}] [G loss: {np.mean(gen_loss):.4f}]\")\n",
        "            # Generate and display images\n",
        "            generate_and_display_images(generator)\n",
        "\n",
        "def generate_and_display_images(generator, num_images=16):\n",
        "    noise = np.random.randn(num_images, 100)\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = 0.5 * generated_images + 0.5  # Rescale images to [0, 1]\n",
        "\n",
        "    fig, axs = plt.subplots(4, 4, figsize=(8, 8))\n",
        "    cnt = 0\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            axs[i, j].imshow(generated_images[cnt, :, :, 0], cmap='gray')\n",
        "            axs[i, j].axis('off')\n",
        "            cnt += 1\n",
        "    plt.show()\n",
        "\n",
        "# Run training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaB2v7YJxc9q"
      },
      "outputs": [],
      "source": [
        "train_gan(epochs=1000, batch_size=64) #change epochs to improve loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "95MEVFfilzAF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}